{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Portfolio Management "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative correlation is a statistical measure used to describe the relationship between two variables. When two variables are negatively correlated, one variable decreases as the other increases, and vice versa. Negative correlations between two investments are used in risk management to diversify, or mitigate, the risk associated with a portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correlations are negative when the prices of the two investments move in different directions.\n",
    "- Risk management is the process of evaluating and mitigating the risks of a portfolio.\n",
    "- Diversifying the portfolio with non-correlated assets can mitigate volatility and risk.\n",
    "- Buying a put option is a tactic used to hedge stocks or portfolios because the put is negatively correlated with the underlying instrument that it is derived from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [Investopedia](https://www.investopedia.com/ask/answers/041315/how-are-negative-correlations-used-risk-management.asp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://plot.ly/scala/line-and-scatter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.191:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1575625654153)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@50a79ec\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "         .builder()\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"Porfolio-Mangement-Risk-Analysis\")\n",
    "         .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------+-----+-----+---------+--------+\n",
      "| Timestamp| Open|     High|  Low|Close|Adj Close|  Volume|\n",
      "+----------+-----+---------+-----+-----+---------+--------+\n",
      "|2014-12-04|   16|16.030001|15.75|15.81|11.765979|23264600|\n",
      "|2014-12-05| 15.9|    15.94|15.63| 15.7|11.684116|29700300|\n",
      "|2014-12-08|15.68|    15.75|15.38|15.43| 11.48318|25519100|\n",
      "|2014-12-09|15.05|    15.43|14.75|15.43| 11.48318|23382800|\n",
      "|2014-12-10|15.43|    15.43|15.15|15.16|11.282244|24615500|\n",
      "|2014-12-11|15.17|    15.49|15.03|15.28|11.371547|30359300|\n",
      "|2014-12-12| 15.2|    15.21|14.99|14.99|11.155725|22684500|\n",
      "|2014-12-15|14.86|    14.92|14.27|14.28|10.627337|47476400|\n",
      "|2014-12-16|14.19|    14.46|13.93|14.09|10.485938|43879300|\n",
      "|2014-12-17|14.17|    14.52|14.11|14.45|10.753853|29504200|\n",
      "|2014-12-18| 14.7|    14.81|14.51|14.81| 11.02177|35542500|\n",
      "|2014-12-19|14.79|    15.05|14.74|15.03|11.185496|40843500|\n",
      "|2014-12-22|15.08|    15.24|   15|15.22|11.326896|25366800|\n",
      "|2014-12-23|15.21|    15.45|15.21|15.33|11.408759|20828700|\n",
      "|2014-12-24|15.37|    15.38|15.25| 15.3|11.386431| 7128800|\n",
      "|2014-12-26| 15.3|    15.49|15.26|15.45|11.498062|11666400|\n",
      "|2014-12-29|15.37|    15.68|15.34|15.52|11.550158|19137500|\n",
      "|2014-12-30|15.48|     15.6|15.45| 15.5|11.535276|14795000|\n",
      "|2014-12-31|15.51|    15.64|15.47| 15.5|11.535276|17930200|\n",
      "|2015-01-02|15.59|    15.65|15.18|15.36|11.431086|24777900|\n",
      "+----------+-----+---------+-----+-----+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_ford: org.apache.spark.sql.DataFrame = [Timestamp: string, Open: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_ford = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./data/F.csv\")\n",
    "df_ford.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+---------+--------+\n",
      "|      Date|     Open|     High|      Low|    Close|Adj Close|  Volume|\n",
      "+----------+---------+---------+---------+---------+---------+--------+\n",
      "|2014-12-04|    33.66|33.669998|33.040001|    33.09|26.576113|11632800|\n",
      "|2014-12-05|    33.23|    34.09|33.169998|    33.93|27.250759|17730800|\n",
      "|2014-12-08|    33.52|33.560001|32.610001|    32.68|26.480963|16125800|\n",
      "|2014-12-09|32.279999|32.869999|32.169998|32.810001|26.586304|11147100|\n",
      "|2014-12-10|32.720001|32.849998|31.870001|31.969999| 25.90564|11583500|\n",
      "|2014-12-11|32.139999|32.689999|    31.99|32.189999|26.083912|13282600|\n",
      "|2014-12-12|31.940001|    32.16|    31.57|    31.57|25.581514|11239100|\n",
      "|2014-12-15|31.700001|    31.77|    30.98|       31|25.119638|16084000|\n",
      "|2014-12-16|30.620001|31.059999|30.299999|    30.73|24.900854|24294000|\n",
      "|2014-12-17|30.809999|31.299999|30.639999|    31.15|25.241186|13281800|\n",
      "|2014-12-18|31.639999|    31.75|    31.17|    31.75|25.727373|14112000|\n",
      "|2014-12-19|    31.76|    32.93|    31.75|32.810001|26.586304|18990900|\n",
      "|2014-12-22|32.630001|33.310001|32.619999|    33.23|26.926632|13243700|\n",
      "|2014-12-23|    33.48|33.700001|33.380001|33.560001|27.194038| 8469500|\n",
      "|2014-12-24|33.529999|    33.59|33.240002|    33.43|27.088696| 4496000|\n",
      "|2014-12-26|33.549999|33.849998|33.450001|    33.73|27.331781| 6673200|\n",
      "|2014-12-29|33.869999|34.830002|33.790001|34.599998|28.036757|16654300|\n",
      "|2014-12-30|34.560001|35.200001|    34.41|    35.09|28.433811|14966900|\n",
      "|2014-12-31|35.240002|35.450001|34.889999|    34.91|28.287951|12004200|\n",
      "|2015-01-02|    35.27|35.310001|    34.41|    34.84|28.231228| 9756700|\n",
      "+----------+---------+---------+---------+---------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_gm: org.apache.spark.sql.DataFrame = [Date: string, Open: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_gm = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./data/GM.csv\")\n",
    "df_gm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "| Timestamp|Ford_Open|\n",
      "+----------+---------+\n",
      "|2014-12-04|       16|\n",
      "|2014-12-05|     15.9|\n",
      "|2014-12-08|    15.68|\n",
      "|2014-12-09|    15.05|\n",
      "|2014-12-10|    15.43|\n",
      "|2014-12-11|    15.17|\n",
      "|2014-12-12|     15.2|\n",
      "|2014-12-15|    14.86|\n",
      "|2014-12-16|    14.19|\n",
      "|2014-12-17|    14.17|\n",
      "|2014-12-18|     14.7|\n",
      "|2014-12-19|    14.79|\n",
      "|2014-12-22|    15.08|\n",
      "|2014-12-23|    15.21|\n",
      "|2014-12-24|    15.37|\n",
      "|2014-12-26|     15.3|\n",
      "|2014-12-29|    15.37|\n",
      "|2014-12-30|    15.48|\n",
      "|2014-12-31|    15.51|\n",
      "|2015-01-02|    15.59|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_ford_open: org.apache.spark.sql.DataFrame = [Timestamp: string, Ford_Open: string]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_ford_open = df_ford.select(\"Timestamp\",\"Open\").withColumnRenamed(\"Open\",\"Ford_Open\")\n",
    "df_ford_open.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      Date|  GM_Open|\n",
      "+----------+---------+\n",
      "|2014-12-04|    33.66|\n",
      "|2014-12-05|    33.23|\n",
      "|2014-12-08|    33.52|\n",
      "|2014-12-09|32.279999|\n",
      "|2014-12-10|32.720001|\n",
      "|2014-12-11|32.139999|\n",
      "|2014-12-12|31.940001|\n",
      "|2014-12-15|31.700001|\n",
      "|2014-12-16|30.620001|\n",
      "|2014-12-17|30.809999|\n",
      "|2014-12-18|31.639999|\n",
      "|2014-12-19|    31.76|\n",
      "|2014-12-22|32.630001|\n",
      "|2014-12-23|    33.48|\n",
      "|2014-12-24|33.529999|\n",
      "|2014-12-26|33.549999|\n",
      "|2014-12-29|33.869999|\n",
      "|2014-12-30|34.560001|\n",
      "|2014-12-31|35.240002|\n",
      "|2015-01-02|    35.27|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_gm_open: org.apache.spark.sql.DataFrame = [Date: string, GM_Open: string]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_gm_open = df_gm.select(\"Date\",\"Open\").withColumnRenamed(\"Open\",\"GM_Open\")\n",
    "df_gm_open.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+\n",
      "| Timestamp|Ford_Open|  GM_Open|\n",
      "+----------+---------+---------+\n",
      "|2014-12-04|       16|    33.66|\n",
      "|2014-12-05|     15.9|    33.23|\n",
      "|2014-12-08|    15.68|    33.52|\n",
      "|2014-12-09|    15.05|32.279999|\n",
      "|2014-12-10|    15.43|32.720001|\n",
      "|2014-12-11|    15.17|32.139999|\n",
      "|2014-12-12|     15.2|31.940001|\n",
      "|2014-12-15|    14.86|31.700001|\n",
      "|2014-12-16|    14.19|30.620001|\n",
      "|2014-12-17|    14.17|30.809999|\n",
      "|2014-12-18|     14.7|31.639999|\n",
      "|2014-12-19|    14.79|    31.76|\n",
      "|2014-12-22|    15.08|32.630001|\n",
      "|2014-12-23|    15.21|    33.48|\n",
      "|2014-12-24|    15.37|33.529999|\n",
      "|2014-12-26|     15.3|33.549999|\n",
      "|2014-12-29|    15.37|33.869999|\n",
      "|2014-12-30|    15.48|34.560001|\n",
      "|2014-12-31|    15.51|35.240002|\n",
      "|2015-01-02|    15.59|    35.27|\n",
      "+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Timestamp: string, Ford_Open: string ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = df_ford_open.as(\"df1\").join(df_gm_open.as(\"df2\"), df_ford_open(\"Timestamp\") === df_gm_open(\"Date\")).select(\"df1.Timestamp\",\"df1.Ford_Open\", \"df2.GM_Open\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.corr\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|corr(Ford_Open, GM_Open)|\n",
      "+------------------------+\n",
      "|    -0.23879596656324004|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(corr(\"Ford_Open\",\"GM_Open\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
